{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019416,
     "end_time": "2020-10-15T02:46:54.547902",
     "exception": false,
     "start_time": "2020-10-15T02:46:54.528486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# README\n",
    "## Run on Kaggle\n",
    "-Make sure to include the following two cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017278,
     "end_time": "2020-10-15T02:46:54.583150",
     "exception": false,
     "start_time": "2020-10-15T02:46:54.565872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Upload the necessary helper files and datasets\n",
    "-GenerateTrainSequences.Py\n",
    "-GlobalConstants.py\n",
    "-HelperFunctions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:46:54.624830Z",
     "iopub.status.busy": "2020-10-15T02:46:54.624041Z",
     "iopub.status.idle": "2020-10-15T02:47:39.536804Z",
     "shell.execute_reply": "2020-10-15T02:47:39.536124Z"
    },
    "papermill": {
     "duration": 44.936378,
     "end_time": "2020-10-15T02:47:39.536926",
     "exception": false,
     "start_time": "2020-10-15T02:46:54.600548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: music21 in /usr/local/lib/python3.6/dist-packages (6.1.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from music21) (0.17.0)\n",
      "Requirement already satisfied: webcolors in /usr/local/lib/python3.6/dist-packages (from music21) (1.11.1)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from music21) (3.0.4)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from music21) (8.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install wget\n",
    "# !pip install music21\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069678,
     "end_time": "2020-10-15T02:47:39.677457",
     "exception": false,
     "start_time": "2020-10-15T02:47:39.607779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download input files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:39.855578Z",
     "iopub.status.busy": "2020-10-15T02:47:39.854584Z",
     "iopub.status.idle": "2020-10-15T02:47:45.383088Z",
     "shell.execute_reply": "2020-10-15T02:47:45.384097Z"
    },
    "papermill": {
     "duration": 5.625458,
     "end_time": "2020-10-15T02:47:45.384315",
     "exception": false,
     "start_time": "2020-10-15T02:47:39.758857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "import zipfile\n",
    "import multiprocessing\n",
    "\n",
    "# def get_inputs( ):\n",
    "#         # Download the datasets from google drive\n",
    "#     filename = 'bach-cwg-64l-32t-cmaj-amin.zip' # update this\n",
    "\n",
    "#     # Update the id, and the filename\n",
    "#     !wget --no-check-certificate 'https://drive.google.com/file/d/1tmqsN5lZ-YsCsF0lfOLMzNb708NLEIpv' -O bach-cwg-64l-32t-cmaj-amin.zip\n",
    "\n",
    "#     # Extract to cwd\n",
    "#     with zipfile.ZipFile(filename,\"r\") as zip_ref:\n",
    "#         zip_ref.extractall(\".\")\n",
    "    \n",
    "# p = multiprocessing.Process(target=get_inputs)\n",
    "# p.start()\n",
    "# p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.106988,
     "end_time": "2020-10-15T02:47:45.604644",
     "exception": false,
     "start_time": "2020-10-15T02:47:45.497656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download model from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:45.847668Z",
     "iopub.status.busy": "2020-10-15T02:47:45.846687Z",
     "iopub.status.idle": "2020-10-15T02:47:45.849788Z",
     "shell.execute_reply": "2020-10-15T02:47:45.849156Z"
    },
    "papermill": {
     "duration": 0.129267,
     "end_time": "2020-10-15T02:47:45.849913",
     "exception": false,
     "start_time": "2020-10-15T02:47:45.720646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Download a large file from Google Drive\n",
    "# #https://github.com/nsadawi/Download-Large-File-From-Google-Drive-Using-Python/blob/master/Download-Large-File-from-Google-Drive.ipynb\n",
    "# #taken from this StackOverflow answer: https://stackoverflow.com/a/39225039\n",
    "# import requests\n",
    "\n",
    "# def get_large_file_from_google_drive():\n",
    "\n",
    "#     def download_file_from_google_drive(id, destination):\n",
    "#         URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "#         session = requests.Session()\n",
    "\n",
    "#         response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "#         token = get_confirm_token(response)\n",
    "\n",
    "#         if token:\n",
    "#             params = { 'id' : id, 'confirm' : token }\n",
    "#             response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "#         save_response_content(response, destination)    \n",
    "\n",
    "#     def get_confirm_token(response):\n",
    "#         for key, value in response.cookies.items():\n",
    "#             if key.startswith('download_warning'):\n",
    "#                 return value\n",
    "\n",
    "#         return None\n",
    "\n",
    "#     def save_response_content(response, destination):\n",
    "#         CHUNK_SIZE = 32768\n",
    "\n",
    "#         with open(destination, \"wb\") as f:\n",
    "#             for chunk in response.iter_content(CHUNK_SIZE):\n",
    "#                 if chunk: # filter out keep-alive new chunks\n",
    "#                     f.write(chunk)\n",
    "\n",
    "#     # Extract the saved model\n",
    "#     file_id = '1dy6GPj4yzhPmVcylCMCxLu472KWltenn'\n",
    "#     destination = './model.zip'\n",
    "#     download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#     filename = \"model.zip\"\n",
    "#     # Extract to cwd\n",
    "#     with zipfile.ZipFile(filename,\"r\") as zip_ref:\n",
    "#         zip_ref.extractall(\".\")\n",
    "        \n",
    "# p = multiprocessing.Process(target=get_large_file_from_google_drive)\n",
    "# p.start()\n",
    "# p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.075654,
     "end_time": "2020-10-15T02:47:45.999383",
     "exception": false,
     "start_time": "2020-10-15T02:47:45.923729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:46.155012Z",
     "iopub.status.busy": "2020-10-15T02:47:46.154170Z",
     "iopub.status.idle": "2020-10-15T02:47:52.390307Z",
     "shell.execute_reply": "2020-10-15T02:47:52.389422Z"
    },
    "papermill": {
     "duration": 6.31771,
     "end_time": "2020-10-15T02:47:52.390424",
     "exception": false,
     "start_time": "2020-10-15T02:47:46.072714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from GlobalConstants import *\n",
    "#import pydot\n",
    "import random\n",
    "from HelperFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:52.564169Z",
     "iopub.status.busy": "2020-10-15T02:47:52.563242Z",
     "iopub.status.idle": "2020-10-15T02:47:58.092154Z",
     "shell.execute_reply": "2020-10-15T02:47:58.093439Z"
    },
    "papermill": {
     "duration": 5.626831,
     "end_time": "2020-10-15T02:47:58.093624",
     "exception": false,
     "start_time": "2020-10-15T02:47:52.466793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: wget: not found\n",
      "unzip:  cannot find or open ngrok-stable-linux-amd64.zip, ngrok-stable-linux-amd64.zip.zip or ngrok-stable-linux-amd64.zip.ZIP.\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/lib/python3.6/json/__init__.py\", line 299, in load\n",
      "    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n",
      "  File \"/usr/lib/python3.6/json/__init__.py\", line 354, in loads\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/usr/lib/python3.6/json/decoder.py\", line 339, in decode\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/usr/lib/python3.6/json/decoder.py\", line 357, in raw_decode\n",
      "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
      "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "# Create Tensorboard\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "!mkdir ./logs/\n",
    "\n",
    "# Download Ngrok to tunnel the tensorboard port to an external port\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip ngrok-stable-linux-amd64.zip\n",
    "\n",
    "# Run tensorboard as well as Ngrox (for tunneling as non-blocking processes)\n",
    "pool = multiprocessing.Pool(processes = 10)\n",
    "results_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n",
    "                        for cmd in [\n",
    "                        f\"tensorboard --logdir ./logs/ --host 0.0.0.0 --port 6006 &\",\n",
    "                        \"./ngrok http 6006 &\"\n",
    "                        ]]\n",
    "\n",
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:58.316903Z",
     "iopub.status.busy": "2020-10-15T02:47:58.315928Z",
     "iopub.status.idle": "2020-10-15T02:47:58.819235Z",
     "shell.execute_reply": "2020-10-15T02:47:58.820684Z"
    },
    "papermill": {
     "duration": 0.621354,
     "end_time": "2020-10-15T02:47:58.820896",
     "exception": false,
     "start_time": "2020-10-15T02:47:58.199542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + \"LOG\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.115035,
     "end_time": "2020-10-15T02:47:59.044324",
     "exception": false,
     "start_time": "2020-10-15T02:47:58.929289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set some initial parameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:59.292809Z",
     "iopub.status.busy": "2020-10-15T02:47:59.291845Z",
     "iopub.status.idle": "2020-10-15T02:47:59.295711Z",
     "shell.execute_reply": "2020-10-15T02:47:59.297116Z"
    },
    "papermill": {
     "duration": 0.129079,
     "end_time": "2020-10-15T02:47:59.297337",
     "exception": false,
     "start_time": "2020-10-15T02:47:59.168258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update this before running\n",
    "TRAIN_SEQUENCE_LENGTH=64\n",
    "\n",
    "INT_SONGS_PATH = os.getcwd()\n",
    "MAPPING_PATH = os.getcwd()\n",
    "SAVE_MODEL_PATH = os.getcwd()\n",
    "\n",
    "S_IDXS = [0, 1, 2, 3, 4, 5, 6]\n",
    "A_IDXS = [7, 8, 9, 10, 11, 12, 13]\n",
    "T_IDXS = [14, 15, 16, 17, 18, 19, 20]\n",
    "B_IDXS = [21, 22, 23, 24, 25, 26, 27]\n",
    "VOCAB_SIZE = [63, None, 15,10,16, 5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113979,
     "end_time": "2020-10-15T02:47:59.521812",
     "exception": false,
     "start_time": "2020-10-15T02:47:59.407833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:59.821971Z",
     "iopub.status.busy": "2020-10-15T02:47:59.795948Z",
     "iopub.status.idle": "2020-10-15T02:47:59.845801Z",
     "shell.execute_reply": "2020-10-15T02:47:59.847025Z"
    },
    "papermill": {
     "duration": 0.210201,
     "end_time": "2020-10-15T02:47:59.847270",
     "exception": false,
     "start_time": "2020-10-15T02:47:59.637069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_sequence_generator_random(int_songs, \n",
    "                                    S_idxs=\"\", \n",
    "                                    A_idxs=\"\", \n",
    "                                    T_idxs=\"\", \n",
    "                                    B_idxs=\"\", \n",
    "                                    vocab_size=\"\", \n",
    "                                    num_train_sequences=1, \n",
    "                                    batch_size=1):\n",
    "    \"\"\"Generate a set of train sequences from a current voice and target. \n",
    "    This instance of the method randomizes the sequences when training\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the sequences\n",
    "  \n",
    "    S_past = [[] for i in range(len(S_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    S_future = [[] for i in range(len(S_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    S_current = [[] for i in range(len(S_idxs))] # (# features, batch_size, )\n",
    "    \n",
    "    B_past = [[] for i in range(len(B_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    B_current = [[] for i in range(len(B_idxs))] # (# features, batch_size, )\n",
    "    \n",
    "    A_past = [[] for i in range(len(A_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    A_current = [[] for i in range(len(A_idxs))] # (# features, batch_size, )\n",
    "    \n",
    "    T_past = [[] for i in range(len(T_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    T_current = [[] for i in range(len(T_idxs))] # (# features, batch_size, )\n",
    "    \n",
    "    S_past_oh = []\n",
    "    S_future_oh=[]\n",
    "    S_current_oh=[]\n",
    "    \n",
    "    B_past_oh = []\n",
    "    B_current_oh = []\n",
    "    \n",
    "    A_past_oh = []\n",
    "    A_current_oh = []\n",
    "    \n",
    "    T_past_oh = []\n",
    "    T_current_oh = []\n",
    "    \n",
    "    batch = 0 # batch counter\n",
    "    k = 0 # counter for keeping track of where we are in int_songs\n",
    "    random_indices = random.sample(range(num_train_sequences),num_train_sequences)\n",
    "\n",
    "    while True:\n",
    "        i = random_indices[k]\n",
    "        \n",
    "        # Create past target melody\n",
    "        for j, idx in enumerate(B_idxs):\n",
    "            B_past[j].append(int_songs[idx][i:i+TRAIN_SEQUENCE_LENGTH])\n",
    "            B_current[j].append(int_songs[idx][i+TRAIN_SEQUENCE_LENGTH])\n",
    "            \n",
    "        for j, idx in enumerate(A_idxs):\n",
    "            A_past[j].append(int_songs[idx][i:i+TRAIN_SEQUENCE_LENGTH])\n",
    "            A_current[j].append(int_songs[idx][i+TRAIN_SEQUENCE_LENGTH])\n",
    "            \n",
    "        for j, idx in enumerate(T_idxs):\n",
    "            T_past[j].append(int_songs[idx][i:i+TRAIN_SEQUENCE_LENGTH])\n",
    "            T_current[j].append(int_songs[idx][i+TRAIN_SEQUENCE_LENGTH])\n",
    "        \n",
    "        for j, idx in enumerate(S_idxs):\n",
    "            # Create past current melody:\n",
    "            S_past[j].append(int_songs[idx][i:i+TRAIN_SEQUENCE_LENGTH])\n",
    "\n",
    "            # Create current note\n",
    "            S_current[j].append(int_songs[idx][i+TRAIN_SEQUENCE_LENGTH])\n",
    "\n",
    "            # Create future current melody\n",
    "            S_future_forward = int_songs[idx][i+1+TRAIN_SEQUENCE_LENGTH:i+1+2*TRAIN_SEQUENCE_LENGTH]\n",
    "            S_future[j].append(S_future_forward[::-1]) # reverse the index\n",
    "\n",
    "        batch+=1\n",
    "\n",
    "        k += 1\n",
    "        # increment i and reset it we reach the number of train sequences\n",
    "        if k == num_train_sequences:\n",
    "            k = 0\n",
    "            random_indices = random.sample(range(num_train_sequences),num_train_sequences)\n",
    "\n",
    "        # yield the batch\n",
    "        if batch == batch_size:\n",
    "            \n",
    "            # one hot encode the vectors\n",
    "            for j, _ in enumerate(B_idxs):\n",
    "                if vocab_size[j]!= None:\n",
    "                    B_past_oh.append(tf.keras.utils.to_categorical(B_past[j], vocab_size[j]))\n",
    "                    B_current_oh.append(tf.keras.utils.to_categorical(B_current[j], vocab_size[j]))\n",
    "                else:\n",
    "                    B_past_oh.append(B_past[j])\n",
    "                    B_current_oh.append(B_current[j])\n",
    "                    \n",
    "            for j, _ in enumerate(A_idxs):\n",
    "                if vocab_size[j]!= None:\n",
    "                    A_past_oh.append(tf.keras.utils.to_categorical(A_past[j], vocab_size[j]))\n",
    "                    A_current_oh.append(tf.keras.utils.to_categorical(A_current[j], vocab_size[j]))\n",
    "                else:\n",
    "                    A_past_oh.append(A_past[j])\n",
    "                    A_current_oh.append(A_current[j])\n",
    "                    \n",
    "            for j, _ in enumerate(T_idxs):\n",
    "                if vocab_size[j]!= None:\n",
    "                    T_past_oh.append(tf.keras.utils.to_categorical(T_past[j], vocab_size[j]))\n",
    "                    T_current_oh.append(tf.keras.utils.to_categorical(T_current[j], vocab_size[j]))\n",
    "                else:\n",
    "                    T_past_oh.append(T_past[j])\n",
    "                    T_current_oh.append(T_current[j])\n",
    "                    \n",
    "            for j, _ in enumerate(S_idxs):\n",
    "                if vocab_size[j]!= None:\n",
    "                    S_past_oh.append(tf.keras.utils.to_categorical(S_past[j], vocab_size[j]))\n",
    "                    S_current_oh.append(tf.keras.utils.to_categorical(S_current[j], vocab_size[j]))\n",
    "                    S_future_oh.append(tf.keras.utils.to_categorical(S_future[j], vocab_size[j]))\n",
    "                else:\n",
    "                    S_past_oh.append(S_past[j])\n",
    "                    S_current_oh.append(S_current[j])\n",
    "                    S_future_oh.append(S_future[j])\n",
    "\n",
    "            #-------------------------------------------------------------\n",
    "            B0_0 = np.array(B_past_oh[0]).astype(np.float) # not sure that this code is needed                        \n",
    "            B2_0 = np.array(B_current_oh[0])\n",
    "            \n",
    "            A0_0 = np.array(A_past_oh[0]).astype(np.float) # not sure that this code is needed                        \n",
    "            A2_0 = np.array(A_current_oh[0])\n",
    "            \n",
    "            T0_0 = np.array(T_past_oh[0]).astype(np.float) # not sure that this code is needed                        \n",
    "            T2_0 = np.array(T_current_oh[0])\n",
    "            \n",
    "            S1_0 = np.array(S_past_oh[0]).astype(np.float)\n",
    "            S2_0 = np.array(S_current_oh[0]).astype(np.float)\n",
    "            S3_0 = np.array(S_future_oh[0]).astype(np.float)\n",
    "            S2_6 = np.array(S_current_oh[6]).astype(np.float)\n",
    "\n",
    "            #------------------------------------------------------------\n",
    "            yield ((B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6), (B2_0, A2_0, T2_0))\n",
    "\n",
    "            # re-initialize the sequences\n",
    "            S_past = [[] for i in range(len(S_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            S_future = [[] for i in range(len(S_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            S_current = [[] for i in range(len(S_idxs))] # (# features, batch_size, )\n",
    "\n",
    "            B_past = [[] for i in range(len(B_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            B_current = [[] for i in range(len(B_idxs))] # (# features, batch_size, )\n",
    "\n",
    "            A_past = [[] for i in range(len(A_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            A_current = [[] for i in range(len(A_idxs))] # (# features, batch_size, )\n",
    "\n",
    "            T_past = [[] for i in range(len(T_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            T_current = [[] for i in range(len(T_idxs))] # (# features, batch_size, )\n",
    "\n",
    "            S_past_oh = []\n",
    "            S_future_oh=[]\n",
    "            S_current_oh=[]\n",
    "\n",
    "            B_past_oh = []\n",
    "            B_current_oh = []\n",
    "\n",
    "            A_past_oh = []\n",
    "            A_current_oh = []\n",
    "\n",
    "            T_past_oh = []\n",
    "            T_current_oh = []\n",
    "\n",
    "            batch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:00.097984Z",
     "iopub.status.busy": "2020-10-15T02:48:00.096871Z",
     "iopub.status.idle": "2020-10-15T02:48:02.465081Z",
     "shell.execute_reply": "2020-10-15T02:48:02.464104Z"
    },
    "papermill": {
     "duration": 2.493583,
     "end_time": "2020-10-15T02:48:02.465231",
     "exception": false,
     "start_time": "2020-10-15T02:47:59.971648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the songs as integers\n",
    "int_songs = open_csv(os.path.join(INT_SONGS_PATH, \"int_songs.csv\")) # Code to test the train sequence generation\n",
    "\n",
    "#### Define constants\n",
    "LOSS = \"categorical_crossentropy\"\n",
    "\n",
    "BATCH_SIZE=64*16 # Numbers between 4-64 work the best \n",
    "\n",
    "train_split = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:02.666561Z",
     "iopub.status.busy": "2020-10-15T02:48:02.665372Z",
     "iopub.status.idle": "2020-10-15T02:48:02.727590Z",
     "shell.execute_reply": "2020-10-15T02:48:02.727037Z"
    },
    "papermill": {
     "duration": 0.184118,
     "end_time": "2020-10-15T02:48:02.727697",
     "exception": false,
     "start_time": "2020-10-15T02:48:02.543579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_samples = len(int_songs[0])\n",
    "num_train_samples = int(train_split*max_samples)\n",
    "int_songs_train = []\n",
    "int_songs_val = []\n",
    "for i,feature in enumerate(int_songs):\n",
    "    int_songs_train.append(int_songs[i][:num_train_samples])\n",
    "    int_songs_val.append(int_songs[i][num_train_samples:max_samples])\n",
    "    \n",
    "NUM_TRAINING_SEQUENCES = len(int_songs_train[0])-2*TRAIN_SEQUENCE_LENGTH\n",
    "NUM_VALIDATION_SEQUENCES = len(int_songs_val[0])-2*TRAIN_SEQUENCE_LENGTH\n",
    "#NUM_TRAINING_SEQUENCES = 10000 # FOR TESTING PURPOSES ONLY (COMMENT OUT THE LINE WHEN RUNNING)\n",
    "\n",
    "TRAINING_STEPS_PER_EPOCH = NUM_TRAINING_SEQUENCES // BATCH_SIZE\n",
    "VALIDATION_STEPS_PER_EPOCH = NUM_VALIDATION_SEQUENCES // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.078687,
     "end_time": "2020-10-15T02:48:02.885229",
     "exception": false,
     "start_time": "2020-10-15T02:48:02.806542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:03.055961Z",
     "iopub.status.busy": "2020-10-15T02:48:03.055368Z",
     "iopub.status.idle": "2020-10-15T02:48:03.477938Z",
     "shell.execute_reply": "2020-10-15T02:48:03.476639Z"
    },
    "papermill": {
     "duration": 0.513278,
     "end_time": "2020-10-15T02:48:03.478085",
     "exception": false,
     "start_time": "2020-10-15T02:48:02.964807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LEN_VOCAB = VOCAB_SIZE[0]\n",
    "\n",
    "# Input Layers\n",
    "B0_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"B0_0\")\n",
    "A0_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"A0_0\")\n",
    "T0_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"T0_0\")\n",
    "S1_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"S1_0\")\n",
    "S2_0 = tf.keras.layers.Input(shape=(VOCAB_SIZE[0]), name=\"S2_0\")\n",
    "S3_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"S3_0\")\n",
    "S2_6 = tf.keras.layers.Input(shape=(VOCAB_SIZE[6]), name=\"S2_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:03.644429Z",
     "iopub.status.busy": "2020-10-15T02:48:03.642424Z",
     "iopub.status.idle": "2020-10-15T02:48:03.645073Z",
     "shell.execute_reply": "2020-10-15T02:48:03.645595Z"
    },
    "papermill": {
     "duration": 0.08638,
     "end_time": "2020-10-15T02:48:03.645719",
     "exception": false,
     "start_time": "2020-10-15T02:48:03.559339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL 2: \"LSTMx128x128-Densex512x128\"\n",
    "# MODEL_NAME = \"LSTMx128x128-Densex512x128-SYMBOL-SCALE-16\"\n",
    "# LSTM1_SIZE = 256\n",
    "\n",
    "# DROPOUT_RATE = 0.5\n",
    "\n",
    "# B = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(B0_0)\n",
    "# B = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(B)\n",
    "# #B = tf.keras.layers.Dropout(DROPOUT_RATE)(B)\n",
    "\n",
    "# A = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(A0_0)\n",
    "# A = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(A)\n",
    "# #A = tf.keras.layers.Dropout(DROPOUT_RATE)(A)\n",
    "\n",
    "# T = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(T0_0)\n",
    "# T = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(T)\n",
    "# #T = tf.keras.layers.Dropout(DROPOUT_RATE)(T)\n",
    "\n",
    "# S1 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S1_0)\n",
    "# S1 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S1)\n",
    "# #S1 = tf.keras.layers.Dropout(DROPOUT_RATE)(S1)\n",
    "\n",
    "# S3 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S3_0)\n",
    "# S3 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S3)\n",
    "# #S3 = tf.keras.layers.Dropout(DROPOUT_RATE)(S3)\n",
    "\n",
    "# d = tf.keras.layers.Concatenate(1)([B,A,T,S1,S3])\n",
    "# d = tf.keras.layers.Flatten()(d)\n",
    "\n",
    "# e = tf.keras.layers.Flatten()(S2_0)\n",
    "# g = tf.keras.layers.Flatten()(S2_6)\n",
    "# f = tf.keras.layers.Concatenate(1)([d,e,g])\n",
    "\n",
    "# f = tf.keras.layers.Dense(1024, activation='relu')(f)\n",
    "# f = tf.keras.layers.Dropout(DROPOUT_RATE)(f)\n",
    "# f = tf.keras.layers.Dense(1024, activation='relu')(f)\n",
    "\n",
    "# B2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(f)\n",
    "# A2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(f)\n",
    "# T2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(f)\n",
    "\n",
    "# model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6],outputs=[B2_0, A2_0, T2_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:03.820141Z",
     "iopub.status.busy": "2020-10-15T02:48:03.819365Z",
     "iopub.status.idle": "2020-10-15T02:48:03.823384Z",
     "shell.execute_reply": "2020-10-15T02:48:03.822803Z"
    },
    "papermill": {
     "duration": 0.092411,
     "end_time": "2020-10-15T02:48:03.823500",
     "exception": false,
     "start_time": "2020-10-15T02:48:03.731089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #MODEL 2: \"LSTMx128x128-Densex512x128\"\n",
    "# MODEL_NAME = \"LSTMx128x128-Densex512x128-SYMBOL-SCALE-16\"\n",
    "# LSTM1_SIZE = 128\n",
    "\n",
    "# DROPOUT_RATE = 0.5\n",
    "\n",
    "# B = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(B0_0)\n",
    "# B = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(B)\n",
    "# #B = tf.keras.layers.Dropout(DROPOUT_RATE)(B)\n",
    "\n",
    "# A = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(A0_0)\n",
    "# A = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(A)\n",
    "# #A = tf.keras.layers.Dropout(DROPOUT_RATE)(A)\n",
    "\n",
    "# T = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(T0_0)\n",
    "# T = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(T)\n",
    "# #T = tf.keras.layers.Dropout(DROPOUT_RATE)(T)\n",
    "\n",
    "# S1 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S1_0)\n",
    "# S1 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S1)\n",
    "# #S1 = tf.keras.layers.Dropout(DROPOUT_RATE)(S1)\n",
    "\n",
    "# S3 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S3_0)\n",
    "# S3 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S3)\n",
    "# #S3 = tf.keras.layers.Dropout(DROPOUT_RATE)(S3)\n",
    "\n",
    "# d = tf.keras.layers.Concatenate(1)([B,A,T,S1,S3])\n",
    "# d = tf.keras.layers.Flatten()(d)\n",
    "\n",
    "# e = tf.keras.layers.Flatten()(S2_0)\n",
    "# g = tf.keras.layers.Flatten()(S2_6)\n",
    "# h = tf.keras.layers.Concatenate(1)([d,e,g])\n",
    "\n",
    "# h = tf.keras.layers.Dense(512, activation='relu')(h)\n",
    "# h = tf.keras.layers.Dropout(DROPOUT_RATE)(h)\n",
    "# h = tf.keras.layers.Dense(128, activation='relu')(h)\n",
    "\n",
    "# B2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='relu')(h)\n",
    "# B_flat = tf.keras.layers.Flatten()(B)\n",
    "# B2_0 = tf.keras.layers.Concatenate(1)([B2_0, B_flat])\n",
    "# B2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(B2_0)\n",
    "\n",
    "# A2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='relu')(h)\n",
    "# A_flat = tf.keras.layers.Flatten()(A)\n",
    "# A2_0 = tf.keras.layers.Concatenate(1)([A2_0, A_flat])\n",
    "# A2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(A2_0)\n",
    "\n",
    "# T2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='relu')(h)\n",
    "# T_flat = tf.keras.layers.Flatten()(T)\n",
    "# T2_0 = tf.keras.layers.Concatenate(1)([T2_0, T_flat])\n",
    "# T2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(T2_0)\n",
    "\n",
    "# model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6],outputs=[B2_0, A2_0, T2_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:04.031731Z",
     "iopub.status.busy": "2020-10-15T02:48:04.007165Z",
     "iopub.status.idle": "2020-10-15T02:48:19.161946Z",
     "shell.execute_reply": "2020-10-15T02:48:19.161388Z"
    },
    "papermill": {
     "duration": 15.257736,
     "end_time": "2020-10-15T02:48:19.162118",
     "exception": false,
     "start_time": "2020-10-15T02:48:03.904382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "build_from_scratch = 1\n",
    "MODEL_NAME = \"LSTMx128x128-Densex512x128-Densex512x128-SYMBOL-SCALE-64-Dropout-0.2-ALL-SCALES\"\n",
    "\n",
    "if build_from_scratch:\n",
    "    LSTM1_SIZE = 128\n",
    "    DENSE1_SIZE = 512\n",
    "    DENSE2_SIZE = 128\n",
    "\n",
    "    DROPOUT_RATE = 0.2\n",
    "\n",
    "    B = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(B0_0)\n",
    "    B = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(B)\n",
    "    #B = tf.keras.layers.Dropout(DROPOUT_RATE)(B)\n",
    "\n",
    "    A = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(A0_0)\n",
    "    A = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(A)\n",
    "    #A = tf.keras.layers.Dropout(DROPOUT_RATE)(A)\n",
    "\n",
    "    T = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(T0_0)\n",
    "    T = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(T)\n",
    "    #T = tf.keras.layers.Dropout(DROPOUT_RATE)(T)\n",
    "\n",
    "    S1 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S1_0)\n",
    "    S1 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S1)\n",
    "    #S1 = tf.keras.layers.Dropout(DROPOUT_RATE)(S1)\n",
    "\n",
    "    S3 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S3_0)\n",
    "    S3 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S3)\n",
    "    #S3 = tf.keras.layers.Dropout(DROPOUT_RATE)(S3)\n",
    "\n",
    "    d = tf.keras.layers.Concatenate(1)([B,A,T,S1,S3])\n",
    "    d = tf.keras.layers.Flatten()(d)\n",
    "\n",
    "    e = tf.keras.layers.Flatten()(S2_0)\n",
    "    g = tf.keras.layers.Flatten()(S2_6)\n",
    "    h = tf.keras.layers.Concatenate(1)([d,e,g])\n",
    "\n",
    "    h = tf.keras.layers.Dense(DENSE1_SIZE, activation='relu')(h)\n",
    "    h = tf.keras.layers.Dropout(DROPOUT_RATE)(h)\n",
    "    h = tf.keras.layers.Dense(DENSE2_SIZE, activation='relu')(h)\n",
    "\n",
    "    B2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='relu')(h)\n",
    "    B_flat = tf.keras.layers.Flatten()(B)\n",
    "    B2_0 = tf.keras.layers.Concatenate(1)([B2_0, B_flat, S2_0, S2_6])\n",
    "    B2_0 = tf.keras.layers.Dense(DENSE1_SIZE, activation='relu')(B2_0)\n",
    "    B2_0 = tf.keras.layers.Dropout(DROPOUT_RATE)(B2_0)\n",
    "    B2_0 = tf.keras.layers.Dense(DENSE2_SIZE, activation='relu')(B2_0)\n",
    "    B2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(B2_0)\n",
    "    B_model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6],outputs=[B2_0])\n",
    "    B_model._name = \"B_model\"\n",
    "\n",
    "    B2_0_1 = tf.keras.layers.Input(shape=(VOCAB_SIZE[0]))\n",
    "    A2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='relu')(h)\n",
    "    A_flat = tf.keras.layers.Flatten()(A)\n",
    "    A2_0 = tf.keras.layers.Concatenate(1)([A2_0, A_flat, S2_0, S2_6, B2_0_1])\n",
    "    A2_0 = tf.keras.layers.Dense(DENSE1_SIZE, activation='relu')(A2_0)\n",
    "    A2_0 = tf.keras.layers.Dropout(DROPOUT_RATE)(A2_0)\n",
    "    A2_0 = tf.keras.layers.Dense(DENSE2_SIZE, activation='relu')(A2_0)\n",
    "    A2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(A2_0)\n",
    "    A_model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6, B2_0_1],outputs=[A2_0])\n",
    "    A_model._name = \"A_model\"\n",
    "\n",
    "    A2_0_1 = tf.keras.layers.Input(shape=(VOCAB_SIZE[0]))\n",
    "    T2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='relu')(h)\n",
    "    T_flat = tf.keras.layers.Flatten()(T)\n",
    "    T2_0 = tf.keras.layers.Concatenate(1)([T2_0, T_flat, S2_0, S2_6, B2_0_1, A2_0_1])\n",
    "    T2_0 = tf.keras.layers.Dense(DENSE1_SIZE, activation='relu')(T2_0)\n",
    "    T2_0 = tf.keras.layers.Dropout(DROPOUT_RATE)(T2_0)\n",
    "    T2_0 = tf.keras.layers.Dense(DENSE2_SIZE, activation='relu')(T2_0)\n",
    "    T2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(T2_0)\n",
    "    T_model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6, B2_0_1, A2_0_1],outputs=[T2_0])\n",
    "    T_model._name = \"T_model\"\n",
    "\n",
    "    B2_0_2 = B_model([B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6])\n",
    "    A2_0_2 = A_model([B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6, B2_0_2])\n",
    "    T2_0_2 = T_model([B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6, B2_0_2, A2_0_2])\n",
    "\n",
    "    model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6],outputs=[B2_0_2, A2_0_2, T2_0_2])\n",
    "    model._name = \"SATB_model\"\n",
    "\n",
    "    # Show basic model statistics\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "else:\n",
    "    model = tf.keras.models.load_model(\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.07893,
     "end_time": "2020-10-15T02:48:19.321706",
     "exception": false,
     "start_time": "2020-10-15T02:48:19.242776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:19.505313Z",
     "iopub.status.busy": "2020-10-15T02:48:19.504065Z",
     "iopub.status.idle": "2020-10-15T02:48:19.515247Z",
     "shell.execute_reply": "2020-10-15T02:48:19.516040Z"
    },
    "papermill": {
     "duration": 0.116639,
     "end_time": "2020-10-15T02:48:19.516174",
     "exception": false,
     "start_time": "2020-10-15T02:48:19.399535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"compiling...\")\n",
    "model.compile(loss=LOSS, \n",
    "              optimizer=tf.keras.optimizers.Adam(lr=.001),\n",
    "              metrics=[\"Accuracy\"])\n",
    "\n",
    "print(\"Creatining the train generator...\")\n",
    "# Create the generator for training\n",
    "train_generator=train_sequence_generator_random(int_songs_train, \n",
    "                                                S_idxs=S_IDXS,\n",
    "                                                A_idxs=A_IDXS,\n",
    "                                                T_idxs=T_IDXS,\n",
    "                                                B_idxs=B_IDXS, \n",
    "                                                vocab_size=VOCAB_SIZE,                                            \n",
    "                                                num_train_sequences=NUM_TRAINING_SEQUENCES,\n",
    "                                                batch_size=BATCH_SIZE)\n",
    "print(\"Creatining the val generator...\")\n",
    "# Create the generator for training\n",
    "val_generator=train_sequence_generator_random(int_songs_val, \n",
    "                                                S_idxs=S_IDXS,\n",
    "                                                A_idxs=A_IDXS,\n",
    "                                                T_idxs=T_IDXS,\n",
    "                                                B_idxs=B_IDXS, \n",
    "                                                vocab_size=VOCAB_SIZE,                                            \n",
    "                                                num_train_sequences=NUM_VALIDATION_SEQUENCES,\n",
    "                                                batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define some parameters for training\n",
    "print(\"Defining training parameters...\")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                              patience=3, min_lr=0.0001, min_delta=1e-2,)\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"mymodel\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        min_delta=1e-3,\n",
    "        patience=5,\n",
    "        verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:19.706127Z",
     "iopub.status.busy": "2020-10-15T02:48:19.687895Z",
     "iopub.status.idle": "2020-10-15T03:06:46.316823Z",
     "shell.execute_reply": "2020-10-15T03:06:46.315982Z"
    },
    "papermill": {
     "duration": 1106.722,
     "end_time": "2020-10-15T03:06:46.316996",
     "exception": false,
     "start_time": "2020-10-15T02:48:19.594996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"fitting the model...\")\n",
    "# Train the mode\n",
    "model.fit(x = train_generator,\n",
    "          steps_per_epoch = TRAINING_STEPS_PER_EPOCH,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          callbacks = [reduce_lr, \n",
    "                       model_checkpoint, \n",
    "                       early_stopping,\n",
    "                       tensorboard_callback\n",
    "                      ],\n",
    "        #  validation_data=val_generator,\n",
    "        #  validation_steps=VALIDATION_STEPS_PER_EPOCH\n",
    "          )\n",
    "\n",
    "print(\"saving the model...\")\n",
    "name = MODEL_NAME + \"{}.h5\".format(time.time())\n",
    "model.save(os.path.join(SAVE_MODEL_PATH, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.113637,
     "end_time": "2020-10-15T03:06:50.864093",
     "exception": false,
     "start_time": "2020-10-15T03:06:48.750456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.164181,
     "end_time": "2020-10-15T03:06:55.160329",
     "exception": false,
     "start_time": "2020-10-15T03:06:52.996148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot the model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T03:06:59.819846Z",
     "iopub.status.busy": "2020-10-15T03:06:59.818814Z",
     "iopub.status.idle": "2020-10-15T03:07:00.607080Z",
     "shell.execute_reply": "2020-10-15T03:07:00.605880Z"
    },
    "papermill": {
     "duration": 3.312042,
     "end_time": "2020-10-15T03:07:00.607247",
     "exception": false,
     "start_time": "2020-10-15T03:06:57.295205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print some basic model statistics\n",
    "history = model.history\n",
    "print(history.history.values())\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy'], loc='upper left')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T03:07:04.938350Z",
     "iopub.status.busy": "2020-10-15T03:07:04.937525Z",
     "iopub.status.idle": "2020-10-15T03:07:05.215075Z",
     "shell.execute_reply": "2020-10-15T03:07:05.214397Z"
    },
    "papermill": {
     "duration": 2.448656,
     "end_time": "2020-10-15T03:07:05.215231",
     "exception": false,
     "start_time": "2020-10-15T03:07:02.766575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "papermill": {
   "duration": 1218.835327,
   "end_time": "2020-10-15T03:07:09.166615",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-15T02:46:50.331288",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
