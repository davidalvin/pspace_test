{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019416,
     "end_time": "2020-10-15T02:46:54.547902",
     "exception": false,
     "start_time": "2020-10-15T02:46:54.528486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# README\n",
    "## Run on Kaggle\n",
    "-Make sure to include the following two cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017278,
     "end_time": "2020-10-15T02:46:54.583150",
     "exception": false,
     "start_time": "2020-10-15T02:46:54.565872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Upload the necessary helper files and datasets\n",
    "-GenerateTrainSequences.Py\n",
    "-GlobalConstants.py\n",
    "-HelperFunctions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:46:54.624830Z",
     "iopub.status.busy": "2020-10-15T02:46:54.624041Z",
     "iopub.status.idle": "2020-10-15T02:47:39.536804Z",
     "shell.execute_reply": "2020-10-15T02:47:39.536124Z"
    },
    "papermill": {
     "duration": 44.936378,
     "end_time": "2020-10-15T02:47:39.536926",
     "exception": false,
     "start_time": "2020-10-15T02:46:54.600548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install wget\n",
    "# !pip install music21\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RUNNING....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.069678,
     "end_time": "2020-10-15T02:47:39.677457",
     "exception": false,
     "start_time": "2020-10-15T02:47:39.607779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download input files from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:39.855578Z",
     "iopub.status.busy": "2020-10-15T02:47:39.854584Z",
     "iopub.status.idle": "2020-10-15T02:47:45.383088Z",
     "shell.execute_reply": "2020-10-15T02:47:45.384097Z"
    },
    "papermill": {
     "duration": 5.625458,
     "end_time": "2020-10-15T02:47:45.384315",
     "exception": false,
     "start_time": "2020-10-15T02:47:39.758857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import wget\n",
    "# import zipfile\n",
    "# import multiprocessing\n",
    "\n",
    "# def get_inputs( ):\n",
    "#         # Download the datasets from google drive\n",
    "#     filename = 'bach-cwg-64l-32t-cmaj-amin.zip' # update this\n",
    "\n",
    "#     # Update the id, and the filename\n",
    "#     !wget --no-check-certificate 'https://drive.google.com/file/d/1tmqsN5lZ-YsCsF0lfOLMzNb708NLEIpv' -O bach-cwg-64l-32t-cmaj-amin.zip\n",
    "\n",
    "#     # Extract to cwd\n",
    "#     with zipfile.ZipFile(filename,\"r\") as zip_ref:\n",
    "#         zip_ref.extractall(\".\")\n",
    "    \n",
    "# p = multiprocessing.Process(target=get_inputs)\n",
    "# p.start()\n",
    "# p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.106988,
     "end_time": "2020-10-15T02:47:45.604644",
     "exception": false,
     "start_time": "2020-10-15T02:47:45.497656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download model from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:45.847668Z",
     "iopub.status.busy": "2020-10-15T02:47:45.846687Z",
     "iopub.status.idle": "2020-10-15T02:47:45.849788Z",
     "shell.execute_reply": "2020-10-15T02:47:45.849156Z"
    },
    "papermill": {
     "duration": 0.129267,
     "end_time": "2020-10-15T02:47:45.849913",
     "exception": false,
     "start_time": "2020-10-15T02:47:45.720646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Download a large file from Google Drive\n",
    "# #https://github.com/nsadawi/Download-Large-File-From-Google-Drive-Using-Python/blob/master/Download-Large-File-from-Google-Drive.ipynb\n",
    "# #taken from this StackOverflow answer: https://stackoverflow.com/a/39225039\n",
    "# import requests\n",
    "\n",
    "# def get_large_file_from_google_drive():\n",
    "\n",
    "#     def download_file_from_google_drive(id, destination):\n",
    "#         URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "#         session = requests.Session()\n",
    "\n",
    "#         response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "#         token = get_confirm_token(response)\n",
    "\n",
    "#         if token:\n",
    "#             params = { 'id' : id, 'confirm' : token }\n",
    "#             response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "#         save_response_content(response, destination)    \n",
    "\n",
    "#     def get_confirm_token(response):\n",
    "#         for key, value in response.cookies.items():\n",
    "#             if key.startswith('download_warning'):\n",
    "#                 return value\n",
    "\n",
    "#         return None\n",
    "\n",
    "#     def save_response_content(response, destination):\n",
    "#         CHUNK_SIZE = 32768\n",
    "\n",
    "#         with open(destination, \"wb\") as f:\n",
    "#             for chunk in response.iter_content(CHUNK_SIZE):\n",
    "#                 if chunk: # filter out keep-alive new chunks\n",
    "#                     f.write(chunk)\n",
    "\n",
    "#     # Extract the saved model\n",
    "#     file_id = '1dy6GPj4yzhPmVcylCMCxLu472KWltenn'\n",
    "#     destination = './model.zip'\n",
    "#     download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "#     filename = \"model.zip\"\n",
    "#     # Extract to cwd\n",
    "#     with zipfile.ZipFile(filename,\"r\") as zip_ref:\n",
    "#         zip_ref.extractall(\".\")\n",
    "        \n",
    "# p = multiprocessing.Process(target=get_large_file_from_google_drive)\n",
    "# p.start()\n",
    "# p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.075654,
     "end_time": "2020-10-15T02:47:45.999383",
     "exception": false,
     "start_time": "2020-10-15T02:47:45.923729",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IMPORTS....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:46.155012Z",
     "iopub.status.busy": "2020-10-15T02:47:46.154170Z",
     "iopub.status.idle": "2020-10-15T02:47:52.390307Z",
     "shell.execute_reply": "2020-10-15T02:47:52.389422Z"
    },
    "papermill": {
     "duration": 6.31771,
     "end_time": "2020-10-15T02:47:52.390424",
     "exception": false,
     "start_time": "2020-10-15T02:47:46.072714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from GlobalConstants import *\n",
    "#import pydot\n",
    "import random\n",
    "from HelperFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.115035,
     "end_time": "2020-10-15T02:47:59.044324",
     "exception": false,
     "start_time": "2020-10-15T02:47:58.929289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set some initial parameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:59.292809Z",
     "iopub.status.busy": "2020-10-15T02:47:59.291845Z",
     "iopub.status.idle": "2020-10-15T02:47:59.295711Z",
     "shell.execute_reply": "2020-10-15T02:47:59.297116Z"
    },
    "papermill": {
     "duration": 0.129079,
     "end_time": "2020-10-15T02:47:59.297337",
     "exception": false,
     "start_time": "2020-10-15T02:47:59.168258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update this before running\n",
    "TRAIN_SEQUENCE_LENGTH=64\n",
    "\n",
    "INT_SONGS_PATH = os.getcwd()\n",
    "MAPPING_PATH = os.getcwd()\n",
    "SAVE_MODEL_PATH = \"/artifacts\"\n",
    "\n",
    "S_IDXS = [0, 1, 2, 3, 4, 5, 6]\n",
    "A_IDXS = [7, 8, 9, 10, 11, 12, 13]\n",
    "T_IDXS = [14, 15, 16, 17, 18, 19, 20]\n",
    "B_IDXS = [21, 22, 23, 24, 25, 26, 27]\n",
    "VOCAB_SIZE = [63, None, 15,10,16, 5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113979,
     "end_time": "2020-10-15T02:47:59.521812",
     "exception": false,
     "start_time": "2020-10-15T02:47:59.407833",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:47:59.821971Z",
     "iopub.status.busy": "2020-10-15T02:47:59.795948Z",
     "iopub.status.idle": "2020-10-15T02:47:59.845801Z",
     "shell.execute_reply": "2020-10-15T02:47:59.847025Z"
    },
    "papermill": {
     "duration": 0.210201,
     "end_time": "2020-10-15T02:47:59.847270",
     "exception": false,
     "start_time": "2020-10-15T02:47:59.637069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_sequence_generator_random(int_songs, \n",
    "                                    S_idxs=\"\", \n",
    "                                    A_idxs=\"\", \n",
    "                                    T_idxs=\"\", \n",
    "                                    B_idxs=\"\", \n",
    "                                    vocab_size=\"\", \n",
    "                                    num_train_sequences=1, \n",
    "                                    batch_size=1):\n",
    "    \"\"\"Generate a set of train sequences from a current voice and target. \n",
    "    This instance of the method randomizes the sequences when training\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the sequences\n",
    "  \n",
    "    S_past = [[] for i in range(len(S_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    S_future = [[] for i in range(len(S_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    S_current = [[] for i in range(len(S_idxs))] # (# features, batch_size, )\n",
    "    \n",
    "    B_past = [[] for i in range(len(B_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    B_current = [[] for i in range(len(B_idxs))] # (# features, batch_size, )\n",
    "    \n",
    "    A_past = [[] for i in range(len(A_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    A_current = [[] for i in range(len(A_idxs))] # (# features, batch_size, )\n",
    "    \n",
    "    T_past = [[] for i in range(len(T_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "    T_current = [[] for i in range(len(T_idxs))] # (# features, batch_size, )\n",
    "    \n",
    "    S_past_oh = []\n",
    "    S_future_oh=[]\n",
    "    S_current_oh=[]\n",
    "    \n",
    "    B_past_oh = []\n",
    "    B_current_oh = []\n",
    "    \n",
    "    A_past_oh = []\n",
    "    A_current_oh = []\n",
    "    \n",
    "    T_past_oh = []\n",
    "    T_current_oh = []\n",
    "    \n",
    "    batch = 0 # batch counter\n",
    "    k = 0 # counter for keeping track of where we are in int_songs\n",
    "    random_indices = random.sample(range(num_train_sequences),num_train_sequences)\n",
    "\n",
    "    while True:\n",
    "        i = random_indices[k]\n",
    "        \n",
    "        # Create past target melody\n",
    "        for j, idx in enumerate(B_idxs):\n",
    "            B_past[j].append(int_songs[idx][i:i+TRAIN_SEQUENCE_LENGTH])\n",
    "            B_current[j].append(int_songs[idx][i+TRAIN_SEQUENCE_LENGTH])\n",
    "            \n",
    "        for j, idx in enumerate(A_idxs):\n",
    "            A_past[j].append(int_songs[idx][i:i+TRAIN_SEQUENCE_LENGTH])\n",
    "            A_current[j].append(int_songs[idx][i+TRAIN_SEQUENCE_LENGTH])\n",
    "            \n",
    "        for j, idx in enumerate(T_idxs):\n",
    "            T_past[j].append(int_songs[idx][i:i+TRAIN_SEQUENCE_LENGTH])\n",
    "            T_current[j].append(int_songs[idx][i+TRAIN_SEQUENCE_LENGTH])\n",
    "        \n",
    "        for j, idx in enumerate(S_idxs):\n",
    "            # Create past current melody:\n",
    "            S_past[j].append(int_songs[idx][i:i+TRAIN_SEQUENCE_LENGTH])\n",
    "\n",
    "            # Create current note\n",
    "            S_current[j].append(int_songs[idx][i+TRAIN_SEQUENCE_LENGTH])\n",
    "\n",
    "            # Create future current melody\n",
    "            S_future_forward = int_songs[idx][i+1+TRAIN_SEQUENCE_LENGTH:i+1+2*TRAIN_SEQUENCE_LENGTH]\n",
    "            S_future[j].append(S_future_forward[::-1]) # reverse the index\n",
    "\n",
    "        batch+=1\n",
    "\n",
    "        k += 1\n",
    "        # increment i and reset it we reach the number of train sequences\n",
    "        if k == num_train_sequences:\n",
    "            k = 0\n",
    "            random_indices = random.sample(range(num_train_sequences),num_train_sequences)\n",
    "\n",
    "        # yield the batch\n",
    "        if batch == batch_size:\n",
    "            \n",
    "            # one hot encode the vectors\n",
    "            for j, _ in enumerate(B_idxs):\n",
    "                if vocab_size[j]!= None:\n",
    "                    B_past_oh.append(tf.keras.utils.to_categorical(B_past[j], vocab_size[j]))\n",
    "                    B_current_oh.append(tf.keras.utils.to_categorical(B_current[j], vocab_size[j]))\n",
    "                else:\n",
    "                    B_past_oh.append(B_past[j])\n",
    "                    B_current_oh.append(B_current[j])\n",
    "                    \n",
    "            for j, _ in enumerate(A_idxs):\n",
    "                if vocab_size[j]!= None:\n",
    "                    A_past_oh.append(tf.keras.utils.to_categorical(A_past[j], vocab_size[j]))\n",
    "                    A_current_oh.append(tf.keras.utils.to_categorical(A_current[j], vocab_size[j]))\n",
    "                else:\n",
    "                    A_past_oh.append(A_past[j])\n",
    "                    A_current_oh.append(A_current[j])\n",
    "                    \n",
    "            for j, _ in enumerate(T_idxs):\n",
    "                if vocab_size[j]!= None:\n",
    "                    T_past_oh.append(tf.keras.utils.to_categorical(T_past[j], vocab_size[j]))\n",
    "                    T_current_oh.append(tf.keras.utils.to_categorical(T_current[j], vocab_size[j]))\n",
    "                else:\n",
    "                    T_past_oh.append(T_past[j])\n",
    "                    T_current_oh.append(T_current[j])\n",
    "                    \n",
    "            for j, _ in enumerate(S_idxs):\n",
    "                if vocab_size[j]!= None:\n",
    "                    S_past_oh.append(tf.keras.utils.to_categorical(S_past[j], vocab_size[j]))\n",
    "                    S_current_oh.append(tf.keras.utils.to_categorical(S_current[j], vocab_size[j]))\n",
    "                    S_future_oh.append(tf.keras.utils.to_categorical(S_future[j], vocab_size[j]))\n",
    "                else:\n",
    "                    S_past_oh.append(S_past[j])\n",
    "                    S_current_oh.append(S_current[j])\n",
    "                    S_future_oh.append(S_future[j])\n",
    "\n",
    "            #-------------------------------------------------------------\n",
    "            B0_0 = np.array(B_past_oh[0]).astype(np.float) # not sure that this code is needed                        \n",
    "            B2_0 = np.array(B_current_oh[0])\n",
    "            \n",
    "            A0_0 = np.array(A_past_oh[0]).astype(np.float) # not sure that this code is needed                        \n",
    "            A2_0 = np.array(A_current_oh[0])\n",
    "            \n",
    "            T0_0 = np.array(T_past_oh[0]).astype(np.float) # not sure that this code is needed                        \n",
    "            T2_0 = np.array(T_current_oh[0])\n",
    "            \n",
    "            S1_0 = np.array(S_past_oh[0]).astype(np.float)\n",
    "            S2_0 = np.array(S_current_oh[0]).astype(np.float)\n",
    "            S3_0 = np.array(S_future_oh[0]).astype(np.float)\n",
    "            S2_6 = np.array(S_current_oh[6]).astype(np.float)\n",
    "\n",
    "            #------------------------------------------------------------\n",
    "            yield ((B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6), (B2_0, A2_0, T2_0))\n",
    "\n",
    "            # re-initialize the sequences\n",
    "            S_past = [[] for i in range(len(S_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            S_future = [[] for i in range(len(S_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            S_current = [[] for i in range(len(S_idxs))] # (# features, batch_size, )\n",
    "\n",
    "            B_past = [[] for i in range(len(B_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            B_current = [[] for i in range(len(B_idxs))] # (# features, batch_size, )\n",
    "\n",
    "            A_past = [[] for i in range(len(A_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            A_current = [[] for i in range(len(A_idxs))] # (# features, batch_size, )\n",
    "\n",
    "            T_past = [[] for i in range(len(T_idxs))] # (# features, batch_size, TRAIN_SEQUENCE_lENGTH)\n",
    "            T_current = [[] for i in range(len(T_idxs))] # (# features, batch_size, )\n",
    "\n",
    "            S_past_oh = []\n",
    "            S_future_oh=[]\n",
    "            S_current_oh=[]\n",
    "\n",
    "            B_past_oh = []\n",
    "            B_current_oh = []\n",
    "\n",
    "            A_past_oh = []\n",
    "            A_current_oh = []\n",
    "\n",
    "            T_past_oh = []\n",
    "            T_current_oh = []\n",
    "\n",
    "            batch = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:00.097984Z",
     "iopub.status.busy": "2020-10-15T02:48:00.096871Z",
     "iopub.status.idle": "2020-10-15T02:48:02.465081Z",
     "shell.execute_reply": "2020-10-15T02:48:02.464104Z"
    },
    "papermill": {
     "duration": 2.493583,
     "end_time": "2020-10-15T02:48:02.465231",
     "exception": false,
     "start_time": "2020-10-15T02:47:59.971648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the songs as integers\n",
    "int_songs = open_csv(os.path.join(INT_SONGS_PATH, \"int_songs.csv\")) # Code to test the train sequence generation\n",
    "\n",
    "#### Define constants\n",
    "LOSS = \"categorical_crossentropy\"\n",
    "\n",
    "BATCH_SIZE=64*4 # Numbers between 4-64 work the best \n",
    "\n",
    "train_split = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:02.666561Z",
     "iopub.status.busy": "2020-10-15T02:48:02.665372Z",
     "iopub.status.idle": "2020-10-15T02:48:02.727590Z",
     "shell.execute_reply": "2020-10-15T02:48:02.727037Z"
    },
    "papermill": {
     "duration": 0.184118,
     "end_time": "2020-10-15T02:48:02.727697",
     "exception": false,
     "start_time": "2020-10-15T02:48:02.543579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_samples = len(int_songs[0])\n",
    "num_train_samples = int(train_split*max_samples)\n",
    "int_songs_train = []\n",
    "int_songs_val = []\n",
    "for i,feature in enumerate(int_songs):\n",
    "    int_songs_train.append(int_songs[i][:num_train_samples])\n",
    "    int_songs_val.append(int_songs[i][num_train_samples:max_samples])\n",
    "    \n",
    "#NUM_TRAINING_SEQUENCES = len(int_songs_train[0])-2*TRAIN_SEQUENCE_LENGTH\n",
    "NUM_VALIDATION_SEQUENCES = len(int_songs_val[0])-2*TRAIN_SEQUENCE_LENGTH\n",
    "NUM_TRAINING_SEQUENCES = 1000 # FOR TESTING PURPOSES ONLY (COMMENT OUT THE LINE WHEN RUNNING)\n",
    "\n",
    "TRAINING_STEPS_PER_EPOCH = NUM_TRAINING_SEQUENCES // BATCH_SIZE\n",
    "VALIDATION_STEPS_PER_EPOCH = NUM_VALIDATION_SEQUENCES // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BUILDING MODEL....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.078687,
     "end_time": "2020-10-15T02:48:02.885229",
     "exception": false,
     "start_time": "2020-10-15T02:48:02.806542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:03.055961Z",
     "iopub.status.busy": "2020-10-15T02:48:03.055368Z",
     "iopub.status.idle": "2020-10-15T02:48:03.477938Z",
     "shell.execute_reply": "2020-10-15T02:48:03.476639Z"
    },
    "papermill": {
     "duration": 0.513278,
     "end_time": "2020-10-15T02:48:03.478085",
     "exception": false,
     "start_time": "2020-10-15T02:48:02.964807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LEN_VOCAB = VOCAB_SIZE[0]\n",
    "\n",
    "# Input Layers\n",
    "B0_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"B0_0\")\n",
    "A0_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"A0_0\")\n",
    "T0_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"T0_0\")\n",
    "S1_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"S1_0\")\n",
    "S2_0 = tf.keras.layers.Input(shape=(VOCAB_SIZE[0]), name=\"S2_0\")\n",
    "S3_0 = tf.keras.layers.Input(shape=(TRAIN_SEQUENCE_LENGTH,VOCAB_SIZE[0]), name=\"S3_0\")\n",
    "S2_6 = tf.keras.layers.Input(shape=(VOCAB_SIZE[6]), name=\"S2_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:04.031731Z",
     "iopub.status.busy": "2020-10-15T02:48:04.007165Z",
     "iopub.status.idle": "2020-10-15T02:48:19.161946Z",
     "shell.execute_reply": "2020-10-15T02:48:19.161388Z"
    },
    "papermill": {
     "duration": 15.257736,
     "end_time": "2020-10-15T02:48:19.162118",
     "exception": false,
     "start_time": "2020-10-15T02:48:03.904382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_3\" was not an Input tensor, it was generated by layer S2_0.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: S2_0:0\n",
      "WARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_3\" was not an Input tensor, it was generated by layer S2_6.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: S2_6:0\n",
      "Model: \"SATB_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "B0_0 (InputLayer)               [(None, 64, 63)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "A0_0 (InputLayer)               [(None, 64, 63)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "T0_0 (InputLayer)               [(None, 64, 63)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "S1_0 (InputLayer)               [(None, 64, 63)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "S2_0 (InputLayer)               [(None, 63)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "S3_0 (InputLayer)               [(None, 64, 63)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "S2_6 (InputLayer)               [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "B_model (Model)                 (None, 63)           26565758    B0_0[0][0]                       \n",
      "                                                                 A0_0[0][0]                       \n",
      "                                                                 T0_0[0][0]                       \n",
      "                                                                 S1_0[0][0]                       \n",
      "                                                                 S2_0[0][0]                       \n",
      "                                                                 S3_0[0][0]                       \n",
      "                                                                 S2_6[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "A_model (Model)                 (None, 63)           26598014    B0_0[0][0]                       \n",
      "                                                                 A0_0[0][0]                       \n",
      "                                                                 T0_0[0][0]                       \n",
      "                                                                 S1_0[0][0]                       \n",
      "                                                                 S2_0[0][0]                       \n",
      "                                                                 S3_0[0][0]                       \n",
      "                                                                 S2_6[0][0]                       \n",
      "                                                                 B_model[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "T_model (Model)                 (None, 63)           26630270    B0_0[0][0]                       \n",
      "                                                                 A0_0[0][0]                       \n",
      "                                                                 T0_0[0][0]                       \n",
      "                                                                 S1_0[0][0]                       \n",
      "                                                                 S2_0[0][0]                       \n",
      "                                                                 S3_0[0][0]                       \n",
      "                                                                 S2_6[0][0]                       \n",
      "                                                                 B_model[1][0]                    \n",
      "                                                                 A_model[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 35,350,138\n",
      "Trainable params: 35,350,138\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "build_from_scratch = 1\n",
    "MODEL_NAME = \"LSTMx128x128-Densex512x128-Densex512x128-SYMBOL-SCALE-64-Dropout-0.2-ALL-SCALES\"\n",
    "\n",
    "if build_from_scratch:\n",
    "    LSTM1_SIZE = 128\n",
    "    DENSE1_SIZE = 512\n",
    "    DENSE2_SIZE = 128\n",
    "\n",
    "    DROPOUT_RATE = 0.2\n",
    "\n",
    "    B = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(B0_0)\n",
    "    B = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(B)\n",
    "    #B = tf.keras.layers.Dropout(DROPOUT_RATE)(B)\n",
    "\n",
    "    A = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(A0_0)\n",
    "    A = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(A)\n",
    "    #A = tf.keras.layers.Dropout(DROPOUT_RATE)(A)\n",
    "\n",
    "    T = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(T0_0)\n",
    "    T = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(T)\n",
    "    #T = tf.keras.layers.Dropout(DROPOUT_RATE)(T)\n",
    "\n",
    "    S1 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S1_0)\n",
    "    S1 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S1)\n",
    "    #S1 = tf.keras.layers.Dropout(DROPOUT_RATE)(S1)\n",
    "\n",
    "    S3 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S3_0)\n",
    "    S3 = tf.keras.layers.LSTM(LSTM1_SIZE, return_sequences=True)(S3)\n",
    "    #S3 = tf.keras.layers.Dropout(DROPOUT_RATE)(S3)\n",
    "\n",
    "    d = tf.keras.layers.Concatenate(1)([B,A,T,S1,S3])\n",
    "    d = tf.keras.layers.Flatten()(d)\n",
    "\n",
    "    e = tf.keras.layers.Flatten()(S2_0)\n",
    "    g = tf.keras.layers.Flatten()(S2_6)\n",
    "    h = tf.keras.layers.Concatenate(1)([d,e,g])\n",
    "\n",
    "    h = tf.keras.layers.Dense(DENSE1_SIZE, activation='relu')(h)\n",
    "    h = tf.keras.layers.Dropout(DROPOUT_RATE)(h)\n",
    "    h = tf.keras.layers.Dense(DENSE2_SIZE, activation='relu')(h)\n",
    "\n",
    "    B2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='relu')(h)\n",
    "    B_flat = tf.keras.layers.Flatten()(B)\n",
    "    B2_0 = tf.keras.layers.Concatenate(1)([B2_0, B_flat, S2_0, S2_6])\n",
    "    B2_0 = tf.keras.layers.Dense(DENSE1_SIZE, activation='relu')(B2_0)\n",
    "    B2_0 = tf.keras.layers.Dropout(DROPOUT_RATE)(B2_0)\n",
    "    B2_0 = tf.keras.layers.Dense(DENSE2_SIZE, activation='relu')(B2_0)\n",
    "    B2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(B2_0)\n",
    "    B_model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6],outputs=[B2_0])\n",
    "    B_model._name = \"B_model\"\n",
    "\n",
    "    B2_0_1 = tf.keras.layers.Input(shape=(VOCAB_SIZE[0]))\n",
    "    A2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='relu')(h)\n",
    "    A_flat = tf.keras.layers.Flatten()(A)\n",
    "    A2_0 = tf.keras.layers.Concatenate(1)([A2_0, A_flat, S2_0, S2_6, B2_0_1])\n",
    "    A2_0 = tf.keras.layers.Dense(DENSE1_SIZE, activation='relu')(A2_0)\n",
    "    A2_0 = tf.keras.layers.Dropout(DROPOUT_RATE)(A2_0)\n",
    "    A2_0 = tf.keras.layers.Dense(DENSE2_SIZE, activation='relu')(A2_0)\n",
    "    A2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(A2_0)\n",
    "    A_model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6, B2_0_1],outputs=[A2_0])\n",
    "    A_model._name = \"A_model\"\n",
    "\n",
    "    A2_0_1 = tf.keras.layers.Input(shape=(VOCAB_SIZE[0]))\n",
    "    T2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='relu')(h)\n",
    "    T_flat = tf.keras.layers.Flatten()(T)\n",
    "    T2_0 = tf.keras.layers.Concatenate(1)([T2_0, T_flat, S2_0, S2_6, B2_0_1, A2_0_1])\n",
    "    T2_0 = tf.keras.layers.Dense(DENSE1_SIZE, activation='relu')(T2_0)\n",
    "    T2_0 = tf.keras.layers.Dropout(DROPOUT_RATE)(T2_0)\n",
    "    T2_0 = tf.keras.layers.Dense(DENSE2_SIZE, activation='relu')(T2_0)\n",
    "    T2_0 = tf.keras.layers.Dense(VOCAB_SIZE[0], activation='softmax')(T2_0)\n",
    "    T_model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6, B2_0_1, A2_0_1],outputs=[T2_0])\n",
    "    T_model._name = \"T_model\"\n",
    "\n",
    "    B2_0_2 = B_model([B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6])\n",
    "    A2_0_2 = A_model([B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6, B2_0_2])\n",
    "    T2_0_2 = T_model([B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6, B2_0_2, A2_0_2])\n",
    "\n",
    "    model = tf.keras.Model(inputs=[B0_0, A0_0, T0_0, S1_0, S2_0, S3_0, S2_6],outputs=[B2_0_2, A2_0_2, T2_0_2])\n",
    "    model._name = \"SATB_model\"\n",
    "\n",
    "    # Show basic model statistics\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "\n",
    "else:\n",
    "    model = tf.keras.models.load_model(\"./model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.07893,
     "end_time": "2020-10-15T02:48:19.321706",
     "exception": false,
     "start_time": "2020-10-15T02:48:19.242776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:19.505313Z",
     "iopub.status.busy": "2020-10-15T02:48:19.504065Z",
     "iopub.status.idle": "2020-10-15T02:48:19.515247Z",
     "shell.execute_reply": "2020-10-15T02:48:19.516040Z"
    },
    "papermill": {
     "duration": 0.116639,
     "end_time": "2020-10-15T02:48:19.516174",
     "exception": false,
     "start_time": "2020-10-15T02:48:19.399535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling...\n",
      "Creatining the train generator...\n",
      "Creatining the val generator...\n",
      "Defining training parameters...\n"
     ]
    }
   ],
   "source": [
    "print(\"compiling...\")\n",
    "model.compile(loss=LOSS, \n",
    "              optimizer=tf.keras.optimizers.Adam(lr=.001),\n",
    "              metrics=[\"Accuracy\"])\n",
    "\n",
    "print(\"Creatining the train generator...\")\n",
    "# Create the generator for training\n",
    "train_generator=train_sequence_generator_random(int_songs_train, \n",
    "                                                S_idxs=S_IDXS,\n",
    "                                                A_idxs=A_IDXS,\n",
    "                                                T_idxs=T_IDXS,\n",
    "                                                B_idxs=B_IDXS, \n",
    "                                                vocab_size=VOCAB_SIZE,                                            \n",
    "                                                num_train_sequences=NUM_TRAINING_SEQUENCES,\n",
    "                                                batch_size=BATCH_SIZE)\n",
    "print(\"Creatining the val generator...\")\n",
    "# Create the generator for training\n",
    "val_generator=train_sequence_generator_random(int_songs_val, \n",
    "                                                S_idxs=S_IDXS,\n",
    "                                                A_idxs=A_IDXS,\n",
    "                                                T_idxs=T_IDXS,\n",
    "                                                B_idxs=B_IDXS, \n",
    "                                                vocab_size=VOCAB_SIZE,                                            \n",
    "                                                num_train_sequences=NUM_VALIDATION_SEQUENCES,\n",
    "                                                batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define some parameters for training\n",
    "print(\"Defining training parameters...\")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                              patience=3, min_lr=0.0001, min_delta=1e-2,)\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"mymodel\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"loss\",\n",
    "        min_delta=1e-3,\n",
    "        patience=5,\n",
    "        verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T02:48:19.706127Z",
     "iopub.status.busy": "2020-10-15T02:48:19.687895Z",
     "iopub.status.idle": "2020-10-15T03:06:46.316823Z",
     "shell.execute_reply": "2020-10-15T03:06:46.315982Z"
    },
    "papermill": {
     "duration": 1106.722,
     "end_time": "2020-10-15T03:06:46.316996",
     "exception": false,
     "start_time": "2020-10-15T02:48:19.594996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting the model...\n",
      "Train for 3 steps\n",
      "2/3 [===================>..........] - ETA: 21s - loss: 10.8516 - B_model_loss: 3.7375 - A_model_loss: 3.5103 - T_model_loss: 3.6038 - B_model_Accuracy: 0.0000e+00 - A_model_Accuracy: 0.0000e+00 - T_model_Accuracy: 0.0000e+00 \n",
      "Epoch 00001: loss improved from inf to 8.52648, saving model to mymodel\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: mymodel/assets\n",
      "3/3 [==============================] - 131s 44s/step - loss: 8.5265 - B_model_loss: 2.9497 - A_model_loss: 2.7677 - T_model_loss: 2.8090 - B_model_Accuracy: 0.0000e+00 - A_model_Accuracy: 0.0000e+00 - T_model_Accuracy: 0.0000e+00\n",
      "saving the model...\n",
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = '/artifacts/LSTMx128x128-Densex512x128-Densex512x128-SYMBOL-SCALE-64-Dropout-0.2-ALL-SCALES1603767685.1119916.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-bc45ddde1bdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saving the model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"{}.h5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \"\"\"\n\u001b[1;32m    974\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m--> 975\u001b[0;31m                       signatures, options)\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    110\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    111\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 112\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/artifacts/LSTMx128x128-Densex512x128-Densex512x128-SYMBOL-SCALE-64-Dropout-0.2-ALL-SCALES1603767685.1119916.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"fitting the model...\")\n",
    "# Train the mode\n",
    "model.fit(x = train_generator,\n",
    "          steps_per_epoch = TRAINING_STEPS_PER_EPOCH,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          callbacks = [reduce_lr, \n",
    "                       model_checkpoint, \n",
    "                       early_stopping,\n",
    "        #               tensorboard_callback\n",
    "                      ],\n",
    "        #  validation_data=val_generator,\n",
    "        #  validation_steps=VALIDATION_STEPS_PER_EPOCH\n",
    "          )\n",
    "\n",
    "print(\"saving the model...\")\n",
    "name = MODEL_NAME + \"{}.h5\".format(time.time())\n",
    "model.save(os.path.join(SAVE_MODEL_PATH, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.113637,
     "end_time": "2020-10-15T03:06:50.864093",
     "exception": false,
     "start_time": "2020-10-15T03:06:48.750456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.164181,
     "end_time": "2020-10-15T03:06:55.160329",
     "exception": false,
     "start_time": "2020-10-15T03:06:52.996148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot the model history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T03:06:59.819846Z",
     "iopub.status.busy": "2020-10-15T03:06:59.818814Z",
     "iopub.status.idle": "2020-10-15T03:07:00.607080Z",
     "shell.execute_reply": "2020-10-15T03:07:00.605880Z"
    },
    "papermill": {
     "duration": 3.312042,
     "end_time": "2020-10-15T03:07:00.607247",
     "exception": false,
     "start_time": "2020-10-15T03:06:57.295205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Print some basic model statistics\n",
    "# history = model.history\n",
    "# print(history.history.values())\n",
    "\n",
    "# plt.subplot(2,1,1)\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['accuracy'], loc='upper left')\n",
    "\n",
    "# plt.subplot(2,1,2)\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.title('loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['loss'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-15T03:07:04.938350Z",
     "iopub.status.busy": "2020-10-15T03:07:04.937525Z",
     "iopub.status.idle": "2020-10-15T03:07:05.215075Z",
     "shell.execute_reply": "2020-10-15T03:07:05.214397Z"
    },
    "papermill": {
     "duration": 2.448656,
     "end_time": "2020-10-15T03:07:05.215231",
     "exception": false,
     "start_time": "2020-10-15T03:07:02.766575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "# plt.grid(True)\n",
    "# plt.gca().set_ylim(0,1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"REACHED END OF FILE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "papermill": {
   "duration": 1218.835327,
   "end_time": "2020-10-15T03:07:09.166615",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-15T02:46:50.331288",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
